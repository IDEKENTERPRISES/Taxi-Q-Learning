{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4049 Assessment 2:\n",
    "\n",
    "This assessment requires the use of a Taxi environment to train a model, using OpenAI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can break down reinforcement learning into five simple steps:__\n",
    "\n",
    "1. The agent is at state zero in an environment.\n",
    "2. It will take an action based on a specific strategy.\n",
    "3. It will receive a reward or punishment based on that action.\n",
    "4. By learning from previous moves the the strategy of the agent becomes optimised. \n",
    "5. The process will repeat until an optimal strategy is found. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon-greedy or $\\epsilon$-greedy method balances the exploration of an environment with a probability $\\epsilon \\approx 10 \\% $ and the exploitation of an environment, with probability $1-\\epsilon$ at the same time. \n",
    "\n",
    "We start with a higher $\\epsilon$, which reduces over time due to understanding the environment better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym  # For the environment.\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\"\"\" Have an agent class, with a policy. \"\"\"\n",
    "from tf_agents.trajectories import TimeStep, Trajectory, trajectory, policy_step\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.policies import py_epsilon_greedy_policy\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym, tf_py_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning Method for Taxi Agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning model uses multiple layers of a neural network to extract the abstract data from the input. \n",
    "\n",
    "The model is made up of 6 components:\n",
    "\n",
    "- Environment\n",
    "  - Reward\n",
    "- Agent\n",
    "  - Policy\n",
    "  - Neural Network\n",
    "- Replay Memory or Buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The initial hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "\n",
    "num_iterations = 10000\n",
    "ALPHA = 0.001 \n",
    "\"\"\" Learning rate \"\"\"\n",
    "EPSILON = 0.6 \n",
    "\"\"\" Exploration-exploitation tradeoff \"\"\"\n",
    "\n",
    "initial_collect_steps = 100\n",
    "\"\"\"Number of times the data collection policy loop is run.\"\"\"\n",
    "collect_steps_per_iteration = 1\n",
    "\n",
    "replay_buffer_max_length = 1000\n",
    "\"\"\"Max length of the replay buffer.\"\"\"\n",
    "\n",
    "batch_size = 1\n",
    "\"\"\"Number of experiences used for each training step.\"\"\"\n",
    "log_interval = 200\n",
    "\"\"\" The amount of times something is logged in the training loop. \"\"\"\n",
    "\n",
    "num_eval_episodes = 50\n",
    "\"\"\"Number of episodes used for the metric.\"\"\"\n",
    "eval_interval = 1000\n",
    "\"\"\"Number of times the evaluation loop is used.\"\"\"\n",
    "fc_layer_params = (60, 10)\n",
    "\"\"\"Parameters for the structure of the neural network.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load the environment into TensorFlow. \"\"\"\n",
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "ACTIONSIZE = env.action_space.n\n",
    "OBSERVATIONSIZE = env.observation_space.n\n",
    "print(ACTIONSIZE, OBSERVATIONSIZE)\n",
    "\n",
    "env = suite_gym.load(env_name)\n",
    "train_py_env = eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "# class RLAgent():\n",
    "#   def __init__(self) -> None:\n",
    "#     self.env_name = 'Taxi-v3'\n",
    "#     \"\"\" self.env = gym.make('Taxi-v3') \"\"\"\n",
    "#     self.env = suite_gym.load(self.env_name)\n",
    "\n",
    "#     train_py_env = suite_gym.load(self.env_name)\n",
    "#     eval_py_env = suite_gym.load(self.env_name)\n",
    "#     train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "#     train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "#     pass\n",
    "#   def policy(self):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " EncodingNetwork (EncodingN  multiple                  730       \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796 (3.11 KB)\n",
      "Trainable params: 796 (3.11 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create the neural network. \"\"\"\n",
    "q_net = q_network.QNetwork(train_env.observation_spec(\n",
    "), train_env.action_spec(), fc_layer_params=fc_layer_params)\n",
    "q_net.create_variables()\n",
    "q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create the agent with the optimiser. \"\"\"\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=ALPHA)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    time_step_spec=train_env.time_step_spec(),\n",
    "    action_spec=train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "\"\"\" print('Observation space:', train_env.observation_spec()) \"\"\"\n",
    "\"\"\" print('Action space:', train_env.action_spec()) \"\"\"\n",
    "\n",
    "\"\"\" print('Collect data spec observation:', agent.collect_data_spec.observation) \"\"\"\n",
    "\"\"\" print('Collect data spec action:', agent.collect_data_spec.action) \"\"\"\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies used are the greedy_policy and the epsilon_greedy_policy.\n",
      "Policy time_step_spec: TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(), dtype=tf.int64, name='observation', minimum=array(0), maximum=array(499)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "Policy action_spec: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(5))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create policies. \"\"\"\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "epGreedyPolicy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n",
    "    agent.policy, epsilon=EPSILON, epsilon_decay_end_count=0.005, epsilon_decay_end_value=0.005)\n",
    "print(\n",
    "    f'Policies used are the {eval_policy.name} and the {collect_policy.name}.', )\n",
    "\n",
    "print('Policy time_step_spec:', eval_policy.time_step_spec)\n",
    "print('Policy action_spec:', eval_policy.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Epsilon Greedy: average return = -203.60000610351562\n",
      "Evaluation policy: greedy_policy average return = -200.0\n",
      "Data Collection Policy: epsilon_greedy_policy average return = -256.5199890136719\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=initial_collect_steps):\n",
    "    \"\"\"Computes the average return for a policy.\"\"\"\n",
    "\n",
    "    total_return = 0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "print(f'True Epsilon Greedy: average return = {compute_avg_return(eval_env, epGreedyPolicy, num_eval_episodes)}')\n",
    "print(\n",
    "    f'Evaluation policy: {eval_policy.name} average return = {compute_avg_return(eval_env, eval_policy, num_eval_episodes)}')\n",
    "print(\n",
    "    f'Data Collection Policy: {collect_policy.name} average return = {compute_avg_return(eval_env, collect_policy, num_eval_episodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# Import the tf_uniform_replay_buffer module\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# Create a TFUniformReplayBuffer instance\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    max_length=replay_buffer_max_length,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Create a ReplayBufferObserver instance\n",
    "rb_observer = [replay_buffer.add_batch]\n",
    "\n",
    "\"\"\" The driver should use the collect policy, or the training policy for exploration.\"\"\"\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=train_env,\n",
    "    policy=epGreedyPolicy,\n",
    "    observers=rb_observer,\n",
    "    num_steps=1\n",
    ")\n",
    "\n",
    "\"\"\" Initial population of a buffer. \"\"\"\n",
    "\n",
    "for _ in range(1000):\n",
    "    driver.run()\n",
    "\n",
    "\"\"\" Convert the buffer into a dataset. \"\"\"\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=5,\n",
    "    sample_batch_size=10).prefetch(3)\n",
    "\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent will output loss every 200 steps.\n",
      "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "Step: 0 \t , Loss: 1.00\n",
      "Step: 200 \t , Loss: 0.27\n",
      "Step: 400 \t , Loss: 1.00\n",
      "Training loss hit 10353.65625\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.2678612>, <tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      "[-200.0, -203.6, -205.4, -200.9]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each episode do:\n",
    "   1. Reset the environment and get the initial state.\n",
    "   2. While the episode is not done:\n",
    "       2.1. Agent chooses an action based on the current state.\n",
    "       2.2. Perform the action in the environment.\n",
    "       2.3. Receive the reward and the next state from the environment\n",
    "       2.4. Store the experience (state, action, reward, next state) in the replay buffer\n",
    "   3. Sample a batch of experiences from the replay buffer.\n",
    "   4. Use the optimizer to update the agent's network parameters based on the sampled experiences.\n",
    "5. Repeat steps 4 until the agent is sufficiently trained.\n",
    "\"\"\"\n",
    "losses = []\n",
    "policies = [compute_avg_return(train_env, agent.policy, 10)]\n",
    "\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Agent will output loss every {log_interval} steps.\")\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    experience = next(iterator)\n",
    "    train_loss = agent.train(experience[0]).loss\n",
    "    \"\"\" if _ % 100 == 0:  # Print every 100 steps\n",
    "    print(f'Step: {_} \\t , Loss: {train_loss}') \"\"\"\n",
    "    if _ % log_interval == 0: \n",
    "        print(f'Step: {_} \\t , Loss: {train_loss:.2f}')\n",
    "        losses.append(train_loss)\n",
    "        policies.append(compute_avg_return(train_env, driver.policy, 10))\n",
    "    if train_loss < 0.005 or train_loss > 10000:\n",
    "        print(f\"Training loss hit {train_loss}\")\n",
    "        break\n",
    "\n",
    "\"\"\" X = range(0, len(losses) * log_interval)\n",
    "Y = losses\n",
    "plt.plot(X, Y)\n",
    "plt.ylim(top=550) \"\"\"\n",
    "\n",
    "print(losses)\n",
    "print(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
