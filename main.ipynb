{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4049 Assessment 2:\n",
    "\n",
    "This assessment requires the use of a Taxi environment to train a model, using OpenAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym  # For the environment.\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can break down reinforcement learning into five simple steps:__\n",
    "\n",
    "1. The agent is at state zero in an environment.\n",
    "2. It will take an action based on a specific strategy.\n",
    "3. It will receive a reward or punishment based on that action.\n",
    "4. By learning from previous moves the the strategy of the agent becomes optimised. \n",
    "5. The process will repeat until an optimal strategy is found. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon-greedy or $\\epsilon$-greedy method balances the exploration of an environment with a probability $\\epsilon \\approx 10 \\% $ and the exploitation of an environment, with probability $1-\\epsilon$ at the same time. \n",
    "\n",
    "We start with a higher $\\epsilon$, which reduces over time due to understanding the environment better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Method for the TaxiAgent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiAgent:\n",
    "    def __init__(self, gamma: float = 0.95, alpha: float = 0.7, currentEpsilon: float = 1.0, decayFactor: float = 0.1):\n",
    "        \"\"\"An agent to be used for the taxi. This will keep track of the state of the taxi. This takes in 4 values, the gamma or the discount factor, the alpha or the learning rate, the current epsilon(the factor that controls the rate of exploration), and the decay factor which controls the rate at which the epsilon reduces.\"\"\"\n",
    "        self.env = gym.make('Taxi-v3')\n",
    "        state_space = self.env.observation_space.n\n",
    "        action_space = self.env.action_space.n\n",
    "        print(state_space, action_space)\n",
    "        self.quality_matrix = np.zeros((state_space, action_space))\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.currentEpsilon = currentEpsilon\n",
    "        self.minEpsilon = decayFactor\n",
    "        self.reset()\n",
    "        \"\"\" print(env.action_space.n) \"\"\"\n",
    "        \"\"\" print(f'Random action = {env.action_space.sample()} ') \"\"\"\n",
    "        \"\"\" print(observation) \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def chooseAction(self, observation) -> int:\n",
    "        \"\"\"Choose the action based on the epsilon greedy principle.\"\"\"\n",
    "        greediness = random.uniform(0, 1)\n",
    "        if greediness > self.currentEpsilon:\n",
    "            # Agent has chosen to exploit the environment\n",
    "            action = np.argmax(self.quality_matrix[observation])\n",
    "        else:\n",
    "            # Agent has chosen to explore the environment\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        self.observation, self.info = self.env.reset()\n",
    "\n",
    "    def updateQualityMatrix(self, action: int, old_obs: int, new_obs: int, reward) -> None:\n",
    "        \"\"\"Internally updates the QMatrix using the Bellman equation.\"\"\"\n",
    "        self.quality_matrix[old_obs][action] += self.alpha*(reward+(self.gamma*np.max(\n",
    "            self.quality_matrix[new_obs]) - self.quality_matrix[old_obs][action]))\n",
    "\n",
    "    def decayEpsilon(self, episode: int) -> None:\n",
    "        \"\"\"A function that changes the epsilon amount to be smaller, reflecting the decrease in exploration.\"\"\"\n",
    "        self.currentEpsilon = self.minEpsilon + \\\n",
    "            (1 - self.minEpsilon)*np.exp(-self.gamma*episode)\n",
    "\n",
    "    def step(self, action) -> bool:\n",
    "        \"\"\"New step function using the QMatrix. Will output True if the environment is terminated or finishes.\"\"\"\n",
    "        new_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.updateQualityMatrix(action, self.observation, new_obs, reward)\n",
    "        self.observation = new_obs\n",
    "        return terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 6\n"
     ]
    }
   ],
   "source": [
    "def train(episodes: int, max_steps: int = 200):\n",
    "    \"\"\"The function to train the TaxiAgent.\"\"\"\n",
    "    agent = TaxiAgent()\n",
    "    for episode in range(episodes):\n",
    "        agent.reset()\n",
    "        agent.decayEpsilon(episode)\n",
    "        curr_step = 1\n",
    "        done = False\n",
    "        while curr_step < max_steps:\n",
    "            action_to_take = agent.chooseAction(agent.observation)\n",
    "            done = agent.step(action_to_take)\n",
    "            curr_step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "resulting_agent = train(2000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward= 8.01 \n",
      " ± std of: 2.62\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(env: gym.Env, max_steps: int, numEvalEpisodes: int, Q: np.array):\n",
    "    \"\"\"This function evaluates the agent environment and outputs the mean reward and the standard deviation reward for the environment.\"\"\"\n",
    "\n",
    "    episode_rewards = []\n",
    "    for episode in range(numEvalEpisodes):\n",
    "        state, _ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take the action (index) that have the maximum reward\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done1, done2, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done1 or done2:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_agent(\n",
    "    resulting_agent.env, 200, 1000, resulting_agent.quality_matrix)\n",
    "print(f\"Mean reward= {mean_reward:.2f} \\n ± std of: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_agent(env: gym.Env, max_steps: int, Q: np.array) -> None:\n",
    "    \"\"\"This is a visualising function for the environment.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    rewards = []\n",
    "    for step in range(max_steps):\n",
    "        # Take the action (index) that have the maximum reward\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done1, done2, info = env.step(action)\n",
    "        total_rewards_ep += reward\n",
    "        rewards.append(total_rewards_ep)\n",
    "\n",
    "        if done1 or done2:\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "\n",
    "new_env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "watch_agent(new_env, 200, resulting_agent.quality_matrix)\n",
    "new_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning Method for Taxi Agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning model uses multiple layers of a neural network to extract the abstract data from an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Have an agent class, with a policy. \"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential, q_network\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy, py_epsilon_greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =   1 # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n",
    "fc_layer_params = (60,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Taxi-v3'\n",
    "env = suite_gym.load(env_name)\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "# class RLAgent():\n",
    "#   def __init__(self) -> None:\n",
    "#     self.env_name = 'Taxi-v3'\n",
    "#     \"\"\" self.env = gym.make('Taxi-v3') \"\"\"\n",
    "#     self.env = suite_gym.load(self.env_name)\n",
    "\n",
    "#     train_py_env = suite_gym.load(self.env_name)\n",
    "#     eval_py_env = suite_gym.load(self.env_name)\n",
    "#     train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "#     train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "#     pass\n",
    "#   def policy(self):\n",
    "#     pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(6,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "\n",
    "q_net = q_network.QNetwork(env.observation_spec(), env.action_spec(), fc_layer_params=fc_layer_params)\n",
    "q_net.create_variables()\n",
    "# dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "# q_values_layer = tf.keras.layers.Dense(\n",
    "#     num_actions,\n",
    "#     activation=None,\n",
    "#     kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "#         minval=-0.03, maxval=0.03),\n",
    "#     bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "\n",
    "\n",
    "# q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-798.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tf_uniform_replay_buffer module\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# Create a TFUniformReplayBuffer instance\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    max_length=replay_buffer_max_length,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create a ReplayBufferObserver instance\n",
    "rb_observer = [replay_buffer.add_batch]\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=env,\n",
    "    policy=eval_policy,\n",
    "    observers=[rb_observer],\n",
    "    num_steps=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(Trajectory(\n",
      "{'action': TensorSpec(shape=(64, 2), dtype=tf.int64, name=None),\n",
      " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
      " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
      " 'observation': TensorSpec(shape=(64, 2), dtype=tf.int64, name=None),\n",
      " 'policy_info': (),\n",
      " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
      " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), BufferInfo(ids=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), probabilities=TensorSpec(shape=(64,), dtype=tf.float32, name=None)))>\n",
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000002403097FE20>\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\main.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m collect_driver \u001b[39m=\u001b[39m dynamic_step_driver\u001b[39m.\u001b[39mDynamicStepDriver(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   env,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   agent\u001b[39m.\u001b[39mcollect_policy,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   [rb_observer],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   num_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   \u001b[39m# Collect a few steps and save to the replay buffer.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   time_step, _ \u001b[39m=\u001b[39m collect_driver\u001b[39m.\u001b[39;49mrun(time_step)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39mprint\u001b[39m(time_step)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juju-/Documents/Repos/ass2/CS4049-assessment-2/main.ipynb#Y103sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   \u001b[39m# Sample a batch of data from the buffer and update the agent's network.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\drivers\\dynamic_step_driver.py:193\u001b[0m, in \u001b[0;36mDynamicStepDriver.run\u001b[1;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, time_step\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, policy_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, maximum_iterations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    177\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Takes steps in the environment using the policy while updating observers.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39m    policy_state: Tensor with final step policy state.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_fn(\n\u001b[0;32m    194\u001b[0m       time_step\u001b[39m=\u001b[39;49mtime_step,\n\u001b[0;32m    195\u001b[0m       policy_state\u001b[39m=\u001b[39;49mpolicy_state,\n\u001b[0;32m    196\u001b[0m       maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[0;32m    197\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\utils\\common.py:193\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m check_tf1_allowed()\n\u001b[0;32m    190\u001b[0m \u001b[39mif\u001b[39;00m has_eager_been_enabled():\n\u001b[0;32m    191\u001b[0m   \u001b[39m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[0;32m    192\u001b[0m   \u001b[39m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m resource_variables_enabled():\n\u001b[0;32m    195\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[1;32mc:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\drivers\\dynamic_step_driver.py:208\u001b[0m, in \u001b[0;36mDynamicStepDriver._run\u001b[1;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[0;32m    205\u001b[0m   policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mget_initial_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m    207\u001b[0m \u001b[39m# Batch dim should be first index of tensors during data collection.\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m batch_dims \u001b[39m=\u001b[39m nest_utils\u001b[39m.\u001b[39;49mget_outer_shape(\n\u001b[0;32m    209\u001b[0m     time_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mtime_step_spec()\n\u001b[0;32m    210\u001b[0m )\n\u001b[0;32m    211\u001b[0m counter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mzeros(batch_dims, tf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m    213\u001b[0m [_, time_step, policy_state] \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    214\u001b[0m     tf\u001b[39m.\u001b[39mstop_gradient,\n\u001b[0;32m    215\u001b[0m     tf\u001b[39m.\u001b[39mwhile_loop(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     ),\n\u001b[0;32m    223\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py:903\u001b[0m, in \u001b[0;36mget_outer_shape\u001b[1;34m(nested_tensor, spec)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[39m# Check tensors have same batch shape.\u001b[39;00m\n\u001b[0;32m    902\u001b[0m num_outer_dims \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(first_tensor\u001b[39m.\u001b[39mshape) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(first_spec\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 903\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched_nested_tensors(\n\u001b[0;32m    904\u001b[0m     nested_tensor, spec, num_outer_dims\u001b[39m=\u001b[39;49mnum_outer_dims, check_dtypes\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    905\u001b[0m ):\n\u001b[0;32m    906\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mconstant([], dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m    908\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mshape(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mfirst_tensor)[:num_outer_dims]\n",
      "File \u001b[1;32mc:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py:527\u001b[0m, in \u001b[0;36mis_batched_nested_tensors\u001b[1;34m(tensors, specs, num_outer_dims, allow_extra_fields, check_dtypes)\u001b[0m\n\u001b[0;32m    524\u001b[0m spec_shapes \u001b[39m=\u001b[39m [spec_shape(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m flat_specs]\n\u001b[0;32m    525\u001b[0m spec_dtypes \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mdtype \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_specs]\n\u001b[1;32m--> 527\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39;49m(s_shape\u001b[39m.\u001b[39;49mrank \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mfor\u001b[39;49;00m s_shape \u001b[39min\u001b[39;49;00m spec_shapes):\n\u001b[0;32m    528\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    529\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mAll specs should have ndims defined.  Saw shapes: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    530\u001b[0m       \u001b[39m%\u001b[39m (tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(specs, spec_shapes),)\n\u001b[0;32m    531\u001b[0m   )\n\u001b[0;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(t_shape\u001b[39m.\u001b[39mrank \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m t_shape \u001b[39min\u001b[39;00m tensor_shapes):\n",
      "File \u001b[1;32mc:\\Users\\juju-\\Documents\\Repos\\ass2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py:527\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m spec_shapes \u001b[39m=\u001b[39m [spec_shape(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m flat_specs]\n\u001b[0;32m    525\u001b[0m spec_dtypes \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mdtype \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_specs]\n\u001b[1;32m--> 527\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(s_shape\u001b[39m.\u001b[39;49mrank \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m s_shape \u001b[39min\u001b[39;00m spec_shapes):\n\u001b[0;32m    528\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    529\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mAll specs should have ndims defined.  Saw shapes: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    530\u001b[0m       \u001b[39m%\u001b[39m (tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(specs, spec_shapes),)\n\u001b[0;32m    531\u001b[0m   )\n\u001b[0;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(t_shape\u001b[39m.\u001b[39mrank \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m t_shape \u001b[39min\u001b[39;00m tensor_shapes):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "agent.train = common.function(agent.train)\n",
    "\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_py_env.reset()\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "  env,\n",
    "  agent.collect_policy,\n",
    "  [rb_observer],\n",
    "  num_steps=1\n",
    ")\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "  time_step, _ = collect_driver.run(time_step)[0]\n",
    "  print(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  # if step % log_interval == 0:\n",
    "  #   print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  # if step % eval_interval == 0:\n",
    "  #   avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "  #   print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "  #   returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
