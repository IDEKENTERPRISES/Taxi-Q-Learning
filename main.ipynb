{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4049 Assessment 2:\n",
    "\n",
    "This assessment requires the use of a Taxi environment to train a model, using OpenAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym  # For the environment.\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can break down reinforcement learning into five simple steps:__\n",
    "\n",
    "1. The agent is at state zero in an environment.\n",
    "2. It will take an action based on a specific strategy.\n",
    "3. It will receive a reward or punishment based on that action.\n",
    "4. By learning from previous moves the the strategy of the agent becomes optimised. \n",
    "5. The process will repeat until an optimal strategy is found. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon-greedy or $\\epsilon$-greedy method balances the exploration of an environment with a probability $\\epsilon \\approx 10 \\% $ and the exploitation of an environment, with probability $1-\\epsilon$ at the same time. \n",
    "\n",
    "We start with a higher $\\epsilon$, which reduces over time due to understanding the environment better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Method for the TaxiAgent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiAgent:\n",
    "    def __init__(self, gamma: float = 0.95, alpha: float = 0.7, currentEpsilon: float = 1.0, decayFactor: float = 0.1):\n",
    "        \"\"\"An agent to be used for the taxi. This will keep track of the state of the taxi. This takes in 4 values, the gamma or the discount factor, the alpha or the learning rate, the current epsilon(the factor that controls the rate of exploration), and the decay factor which controls the rate at which the epsilon reduces.\"\"\"\n",
    "        self.env = gym.make('Taxi-v3')\n",
    "        state_space = self.env.observation_space.n\n",
    "        action_space = self.env.action_space.n\n",
    "        print(state_space, action_space)\n",
    "        self.quality_matrix = np.zeros((state_space, action_space))\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.currentEpsilon = currentEpsilon\n",
    "        self.minEpsilon = decayFactor\n",
    "        self.reset()\n",
    "        \"\"\" print(env.action_space.n) \"\"\"\n",
    "        \"\"\" print(f'Random action = {env.action_space.sample()} ') \"\"\"\n",
    "        \"\"\" print(observation) \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def chooseAction(self, observation) -> int:\n",
    "        \"\"\"Choose the action based on the epsilon greedy principle.\"\"\"\n",
    "        greediness = random.uniform(0, 1)\n",
    "        if greediness > self.currentEpsilon:\n",
    "            # Agent has chosen to exploit the environment\n",
    "            action = np.argmax(self.quality_matrix[observation])\n",
    "        else:\n",
    "            # Agent has chosen to explore the environment\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        self.observation, self.info = self.env.reset()\n",
    "\n",
    "    def updateQualityMatrix(self, action: int, old_obs: int, new_obs: int, reward) -> None:\n",
    "        \"\"\"Internally updates the QMatrix using the Bellman equation.\"\"\"\n",
    "        self.quality_matrix[old_obs][action] += self.alpha*(reward+(self.gamma*np.max(\n",
    "            self.quality_matrix[new_obs]) - self.quality_matrix[old_obs][action]))\n",
    "\n",
    "    def decayEpsilon(self, episode: int) -> None:\n",
    "        \"\"\"A function that changes the epsilon amount to be smaller, reflecting the decrease in exploration.\"\"\"\n",
    "        self.currentEpsilon = self.minEpsilon + \\\n",
    "            (1 - self.minEpsilon)*np.exp(-self.gamma*episode)\n",
    "\n",
    "    def step(self, action) -> bool:\n",
    "        \"\"\"New step function using the QMatrix. Will output True if the environment is terminated or finishes.\"\"\"\n",
    "        new_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.updateQualityMatrix(action, self.observation, new_obs, reward)\n",
    "        self.observation = new_obs\n",
    "        return terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes: int, max_steps: int = 200):\n",
    "    \"\"\"The function to train the TaxiAgent.\"\"\"\n",
    "    agent = TaxiAgent()\n",
    "    for episode in range(episodes):\n",
    "        agent.reset()\n",
    "        agent.decayEpsilon(episode)\n",
    "        curr_step = 1\n",
    "        done = False\n",
    "        while curr_step < max_steps:\n",
    "            action_to_take = agent.chooseAction(agent.observation)\n",
    "            done = agent.step(action_to_take)\n",
    "            curr_step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "resulting_agent = train(2000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env: gym.Env, max_steps: int, numEvalEpisodes: int, Q: np.array):\n",
    "    \"\"\"This function evaluates the agent environment and outputs the mean reward and the standard deviation reward for the environment.\"\"\"\n",
    "\n",
    "    episode_rewards = []\n",
    "    for episode in range(numEvalEpisodes):\n",
    "        state, _ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take the action (index) that have the maximum reward\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done1, done2, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done1 or done2:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_agent(\n",
    "    resulting_agent.env, 200, 1000, resulting_agent.quality_matrix)\n",
    "print(f\"Mean reward= {mean_reward:.2f} \\n Â± std of: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_agent(env: gym.Env, max_steps: int, Q: np.array) -> None:\n",
    "    \"\"\"This is a visualising function for the environment.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    rewards = []\n",
    "    for step in range(max_steps):\n",
    "        # Take the action (index) that have the maximum reward\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done1, done2, info = env.step(action)\n",
    "        total_rewards_ep += reward\n",
    "        rewards.append(total_rewards_ep)\n",
    "\n",
    "        if done1 or done2:\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "\n",
    "new_env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "watch_agent(new_env, 200, resulting_agent.quality_matrix)\n",
    "new_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning Method for Taxi Agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning model uses multiple layers of a neural network to extract the abstract data from the input. \n",
    "\n",
    "The model is made up of 6 components:\n",
    "\n",
    "- Environment\n",
    "  - Reward\n",
    "- Agent\n",
    "  - Policy\n",
    "  - Neural Network\n",
    "- Replay Memory or Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Have an agent class, with a policy. \"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import suite_gym, tf_py_environment\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential, q_network\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy, py_epsilon_greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.trajectories import TimeStep, Trajectory, trajectory, policy_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "num_iterations = 10000\n",
    "\"\"\" Learning rate \"\"\"\n",
    "ALPHA = 0.001 \n",
    "\"\"\" Exploration-exploitation tradeoff \"\"\"\n",
    "EPSILON = 0.6 \n",
    "\"\"\"Discount factor \"\"\"\n",
    "\n",
    "initial_collect_steps = 100\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 1000\n",
    "\n",
    "batch_size = 1\n",
    "\"\"\" The amount of times something is logged in the training loop. \"\"\"\n",
    "log_interval = 200\n",
    "\n",
    "num_eval_episodes = 50\n",
    "eval_interval = 1000\n",
    "fc_layer_params = (60, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load the environment into TensorFlow. \"\"\"\n",
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "ACTIONSIZE = env.action_space.n\n",
    "OBSERVATIONSIZE = env.observation_space.n\n",
    "print(ACTIONSIZE, OBSERVATIONSIZE)\n",
    "\n",
    "env = suite_gym.load(env_name)\n",
    "train_py_env = eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "\n",
    "# class RLAgent():\n",
    "#   def __init__(self) -> None:\n",
    "#     self.env_name = 'Taxi-v3'\n",
    "#     \"\"\" self.env = gym.make('Taxi-v3') \"\"\"\n",
    "#     self.env = suite_gym.load(self.env_name)\n",
    "\n",
    "#     train_py_env = suite_gym.load(self.env_name)\n",
    "#     eval_py_env = suite_gym.load(self.env_name)\n",
    "#     train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "#     train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "#     pass\n",
    "#   def policy(self):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " EncodingNetwork (EncodingN  multiple                  730       \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " dense_84 (Dense)            multiple                  66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796 (3.11 KB)\n",
      "Trainable params: 796 (3.11 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create the neural network. \"\"\"\n",
    "q_net = q_network.QNetwork(train_env.observation_spec(\n",
    "), train_env.action_spec(), fc_layer_params=fc_layer_params)\n",
    "q_net.create_variables()\n",
    "q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create the agent with the optimiser. \"\"\"\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=ALPHA)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    time_step_spec=train_env.time_step_spec(),\n",
    "    action_spec=train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "\"\"\" print('Observation space:', train_env.observation_spec()) \"\"\"\n",
    "\"\"\" print('Action space:', train_env.action_spec()) \"\"\"\n",
    "\n",
    "\"\"\" print('Collect data spec observation:', agent.collect_data_spec.observation) \"\"\"\n",
    "\"\"\" print('Collect data spec action:', agent.collect_data_spec.action) \"\"\"\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies used are the greedy_policy and the epsilon_greedy_policy.\n",
      "Policy time_step_spec: TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(), dtype=tf.int64, name='observation', minimum=array(0), maximum=array(499)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "Policy action_spec: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(5))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create policies. \"\"\"\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "epGreedyPolicy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n",
    "    agent.policy, epsilon=EPSILON, epsilon_decay_end_count=0.005, epsilon_decay_end_value=0.005)\n",
    "print(\n",
    "    f'Policies used are the {eval_policy.name} and the {collect_policy.name}.', )\n",
    "\n",
    "print('Policy time_step_spec:', eval_policy.time_step_spec)\n",
    "print('Policy action_spec:', eval_policy.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Epsilon Greedy: average return = -203.05999755859375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_return\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrue Epsilon Greedy: average return = \u001b[39m\u001b[39m{\u001b[39;00mcompute_avg_return(eval_env,\u001b[39m \u001b[39mepGreedyPolicy,\u001b[39m \u001b[39mnum_eval_episodes)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEvaluation policy: \u001b[39m\u001b[39m{\u001b[39;00meval_policy\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m average return = \u001b[39m\u001b[39m{\u001b[39;00mcompute_avg_return(eval_env,\u001b[39m \u001b[39;49meval_policy,\u001b[39m \u001b[39;49mnum_eval_episodes)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData Collection Policy: \u001b[39m\u001b[39m{\u001b[39;00mcollect_policy\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m average return = \u001b[39m\u001b[39m{\u001b[39;00mcompute_avg_return(eval_env,\u001b[39m \u001b[39mcollect_policy,\u001b[39m \u001b[39mnum_eval_episodes)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m episode_return \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m time_step\u001b[39m.\u001b[39mis_last():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     action_step \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49maction(time_step)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     time_step \u001b[39m=\u001b[39m environment\u001b[39m.\u001b[39mstep(action_step\u001b[39m.\u001b[39maction)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     episode_return \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time_step\u001b[39m.\u001b[39mreward\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/policies/tf_policy.py:333\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_automatic_state_reset:\n\u001b[1;32m    332\u001b[0m   policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[0;32m--> 333\u001b[0m step \u001b[39m=\u001b[39m action_fn(time_step\u001b[39m=\u001b[39;49mtime_step, policy_state\u001b[39m=\u001b[39;49mpolicy_state, seed\u001b[39m=\u001b[39;49mseed)\n\u001b[1;32m    335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclip_action\u001b[39m(action, action_spec):\n\u001b[1;32m    336\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(action_spec, tensor_spec\u001b[39m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/utils/common.py:193\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m check_tf1_allowed()\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    191\u001b[0m   \u001b[39m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    192\u001b[0m   \u001b[39m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    195\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/policies/tf_policy.py:589\u001b[0m, in \u001b[0;36mTFPolicy._action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implementation of `action`.\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \n\u001b[1;32m    576\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39m    `info`: Optional side information such as action log probabilities.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    588\u001b[0m seed_stream \u001b[39m=\u001b[39m tfp\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mSeedStream(seed\u001b[39m=\u001b[39mseed, salt\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf_agents_tf_policy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 589\u001b[0m distribution_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution(time_step, policy_state)  \u001b[39m# pytype: disable=wrong-arg-types\u001b[39;00m\n\u001b[1;32m    590\u001b[0m actions \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    591\u001b[0m     \u001b[39mlambda\u001b[39;00m d: reparameterized_sampling\u001b[39m.\u001b[39msample(d, seed\u001b[39m=\u001b[39mseed_stream()),\n\u001b[1;32m    592\u001b[0m     distribution_step\u001b[39m.\u001b[39maction,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m info \u001b[39m=\u001b[39m distribution_step\u001b[39m.\u001b[39minfo\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/policies/greedy_policy.py:82\u001b[0m, in \u001b[0;36mGreedyPolicy._distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     76\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYour network\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms distribution does not implement mode \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaking it incompatible with a greedy policy.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     78\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m     80\u001b[0m   \u001b[39mreturn\u001b[39;00m tfp\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mDeterministic(loc\u001b[39m=\u001b[39mgreedy_action)\n\u001b[0;32m---> 82\u001b[0m distribution_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_policy\u001b[39m.\u001b[39;49mdistribution(\n\u001b[1;32m     83\u001b[0m     time_step, policy_state\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m policy_step\u001b[39m.\u001b[39mPolicyStep(\n\u001b[1;32m     86\u001b[0m     tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(dist_fn, distribution_step\u001b[39m.\u001b[39maction),\n\u001b[1;32m     87\u001b[0m     distribution_step\u001b[39m.\u001b[39mstate,\n\u001b[1;32m     88\u001b[0m     distribution_step\u001b[39m.\u001b[39minfo,\n\u001b[1;32m     89\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/policies/tf_policy.py:422\u001b[0m, in \u001b[0;36mTFPolicy.distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_automatic_state_reset:\n\u001b[1;32m    421\u001b[0m   policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[0;32m--> 422\u001b[0m step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution(time_step\u001b[39m=\u001b[39;49mtime_step, policy_state\u001b[39m=\u001b[39;49mpolicy_state)\n\u001b[1;32m    423\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memit_log_probability:\n\u001b[1;32m    424\u001b[0m   \u001b[39m# This here is set only for compatibility with info_spec in constructor.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m   info \u001b[39m=\u001b[39m policy_step\u001b[39m.\u001b[39mset_log_probability(\n\u001b[1;32m    426\u001b[0m       step\u001b[39m.\u001b[39minfo,\n\u001b[1;32m    427\u001b[0m       tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m       ),\n\u001b[1;32m    431\u001b[0m   )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/policies/q_policy.py:162\u001b[0m, in \u001b[0;36mQPolicy._distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m observation_and_action_constraint_splitter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m   network_observation, mask \u001b[39m=\u001b[39m observation_and_action_constraint_splitter(\n\u001b[1;32m    159\u001b[0m       network_observation\n\u001b[1;32m    160\u001b[0m   )\n\u001b[0;32m--> 162\u001b[0m q_values, policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_q_network(\n\u001b[1;32m    163\u001b[0m     network_observation,\n\u001b[1;32m    164\u001b[0m     network_state\u001b[39m=\u001b[39;49mpolicy_state,\n\u001b[1;32m    165\u001b[0m     step_type\u001b[39m=\u001b[39;49mtime_step\u001b[39m.\u001b[39;49mstep_type,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    168\u001b[0m logits \u001b[39m=\u001b[39m q_values\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m observation_and_action_constraint_splitter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   \u001b[39m# Overwrite the logits for invalid actions to logits.dtype.min.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tf_agents/networks/network.py:408\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_tensor_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m   nest_utils\u001b[39m.\u001b[39massert_matching_dtypes_and_inner_shapes(\n\u001b[1;32m    400\u001b[0m       inputs,\n\u001b[1;32m    401\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_tensor_spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m       specs_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`input_tensor_spec`\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    406\u001b[0m   )\n\u001b[0;32m--> 408\u001b[0m call_argspec \u001b[39m=\u001b[39m tf_inspect\u001b[39m.\u001b[39;49mgetargspec(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall)\n\u001b[1;32m    410\u001b[0m \u001b[39m# Convert *args, **kwargs to a canonical kwarg representation.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m normalized_kwargs \u001b[39m=\u001b[39m tf_inspect\u001b[39m.\u001b[39mgetcallargs(\n\u001b[1;32m    412\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall, inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    413\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tensorflow/python/util/tf_inspect.py:160\u001b[0m, in \u001b[0;36mgetargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[39mreturn\u001b[39;00m spec\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[39m# Python3 will handle most callables here (not partial).\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   \u001b[39mreturn\u001b[39;00m _getargspec(target)\n\u001b[1;32m    161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tensorflow/python/util/tf_inspect.py:92\u001b[0m, in \u001b[0;36m_getargspec\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getargspec\u001b[39m(target):\n\u001b[1;32m     77\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"A python3 version of getargspec.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[39m  Calls `getfullargspec` and assigns args, varargs,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m    from FullArgSpec.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m   fullargspecs \u001b[39m=\u001b[39m getfullargspec(target)\n\u001b[1;32m     94\u001b[0m   defaults \u001b[39m=\u001b[39m fullargspecs\u001b[39m.\u001b[39mdefaults \u001b[39mor\u001b[39;00m ()\n\u001b[1;32m     95\u001b[0m   \u001b[39mif\u001b[39;00m fullargspecs\u001b[39m.\u001b[39mkwonlydefaults:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/tensorflow/python/util/tf_inspect.py:286\u001b[0m, in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[39mif\u001b[39;00m d\u001b[39m.\u001b[39mdecorator_argspec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_maybe_argspec_to_fullargspec(d\u001b[39m.\u001b[39mdecorator_argspec)\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m _getfullargspec(target)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/inspect.py:1369\u001b[0m, in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get the names and default values of a callable object's parameters.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \n\u001b[1;32m   1339\u001b[0m \u001b[39mA tuple of seven things is returned:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[39m  - wrapper chains defined by __wrapped__ *not* unwrapped automatically\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m     \u001b[39m# Re: `skip_bound_arg=False`\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[39m# getfullargspec() historically ignored __wrapped__ attributes,\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m     \u001b[39m# so we ensure that remains the case in 3.3+\u001b[39;00m\n\u001b[0;32m-> 1369\u001b[0m     sig \u001b[39m=\u001b[39m _signature_from_callable(func,\n\u001b[1;32m   1370\u001b[0m                                    follow_wrapper_chains\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1371\u001b[0m                                    skip_bound_arg\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1372\u001b[0m                                    sigcls\u001b[39m=\u001b[39;49mSignature,\n\u001b[1;32m   1373\u001b[0m                                    eval_str\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1374\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   1375\u001b[0m     \u001b[39m# Most of the times 'signature' will raise ValueError.\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m     \u001b[39m# But, it can also raise AttributeError, and, maybe something\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     \u001b[39m# else. So to be fully backwards compatible, we catch all\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m     \u001b[39m# possible exceptions here, and reraise a TypeError.\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39munsupported callable\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/inspect.py:2454\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is not a callable object\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(obj))\n\u001b[1;32m   2451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, types\u001b[39m.\u001b[39mMethodType):\n\u001b[1;32m   2452\u001b[0m     \u001b[39m# In this case we skip the first parameter of the underlying\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m     \u001b[39m# function (usually `self` or `cls`).\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m     sig \u001b[39m=\u001b[39m _get_signature_of(obj\u001b[39m.\u001b[39;49m\u001b[39m__func__\u001b[39;49m)\n\u001b[1;32m   2456\u001b[0m     \u001b[39mif\u001b[39;00m skip_bound_arg:\n\u001b[1;32m   2457\u001b[0m         \u001b[39mreturn\u001b[39;00m _signature_bound_method(sig)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/inspect.py:2513\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2510\u001b[0m             new_params \u001b[39m=\u001b[39m (first_wrapped_param,) \u001b[39m+\u001b[39m sig_params\n\u001b[1;32m   2511\u001b[0m             \u001b[39mreturn\u001b[39;00m sig\u001b[39m.\u001b[39mreplace(parameters\u001b[39m=\u001b[39mnew_params)\n\u001b[0;32m-> 2513\u001b[0m \u001b[39mif\u001b[39;00m isfunction(obj) \u001b[39mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2514\u001b[0m     \u001b[39m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2515\u001b[0m     \u001b[39m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[1;32m   2516\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2517\u001b[0m                                     skip_bound_arg\u001b[39m=\u001b[39mskip_bound_arg,\n\u001b[1;32m   2518\u001b[0m                                     \u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mglobals\u001b[39m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlocals\u001b[39m, eval_str\u001b[39m=\u001b[39meval_str)\n\u001b[1;32m   2520\u001b[0m \u001b[39mif\u001b[39;00m _signature_is_builtin(obj):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/inspect.py:378\u001b[0m, in \u001b[0;36misfunction\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"Return true if the object is a getset descriptor.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \n\u001b[1;32m    374\u001b[0m \u001b[39m        getset descriptors are specialized descriptors defined in extension\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[39m        modules.\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misfunction\u001b[39m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m    379\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return true if the object is a user-defined function.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39m    Function objects provide these attributes:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m        __annotations__ dict of parameter annotations\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m        __kwdefaults__  dict of keyword only parameters with defaults\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mobject\u001b[39m, types\u001b[39m.\u001b[39mFunctionType)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=initial_collect_steps):\n",
    "    \"\"\"Computes the average return for a policy.\"\"\"\n",
    "\n",
    "    total_return = 0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "print(f'True Epsilon Greedy: average return = {compute_avg_return(eval_env, epGreedyPolicy, num_eval_episodes)}')\n",
    "print(\n",
    "    f'Evaluation policy: {eval_policy.name} average return = {compute_avg_return(eval_env, eval_policy, num_eval_episodes)}')\n",
    "print(\n",
    "    f'Data Collection Policy: {collect_policy.name} average return = {compute_avg_return(eval_env, collect_policy, num_eval_episodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tf_uniform_replay_buffer module\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# Create a TFUniformReplayBuffer instance\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    max_length=replay_buffer_max_length,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Create a ReplayBufferObserver instance\n",
    "rb_observer = [replay_buffer.add_batch]\n",
    "\n",
    "\"\"\" The driver should use the collect policy, or the training policy for exploration.\"\"\"\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=train_env,\n",
    "    policy=epGreedyPolicy,\n",
    "    observers=rb_observer,\n",
    "    num_steps=1\n",
    ")\n",
    "\n",
    "\"\"\" Initial population of a buffer. \"\"\"\n",
    "\n",
    "for _ in range(1000):\n",
    "    driver.run()\n",
    "\n",
    "\"\"\" Convert the buffer into a dataset. \"\"\"\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=5,\n",
    "    sample_batch_size=10).prefetch(3)\n",
    "\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent will output loss every 200 steps.\n",
      "Step: 0 \t , Loss: 0.01\n",
      "Training loss hit 0.004804062657058239\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (200,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(losses) \u001b[39m*\u001b[39m log_interval)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m Y \u001b[39m=\u001b[39m losses\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X35sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(X, Y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m plt\u001b[39m.\u001b[39mylim(top\u001b[39m=\u001b[39m\u001b[39m550\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harryk/Documents/uni/year3/cs4049/cs4049-assessment-2/main.ipynb#X35sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(losses)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/matplotlib/pyplot.py:3578\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   3571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\n\u001b[1;32m   3572\u001b[0m     \u001b[39m*\u001b[39margs: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m ArrayLike \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3577\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3578\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   3579\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m   3580\u001b[0m         scalex\u001b[39m=\u001b[39;49mscalex,\n\u001b[1;32m   3581\u001b[0m         scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   3582\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   3583\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3584\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taxi/lib/python3.11/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (200,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each episode do:\n",
    "   1. Reset the environment and get the initial state.\n",
    "   2. While the episode is not done:\n",
    "       2.1. Agent chooses an action based on the current state.\n",
    "       2.2. Perform the action in the environment.\n",
    "       2.3. Receive the reward and the next state from the environment\n",
    "       2.4. Store the experience (state, action, reward, next state) in the replay buffer\n",
    "   3. Sample a batch of experiences from the replay buffer.\n",
    "   4. Use the optimizer to update the agent's network parameters based on the sampled experiences.\n",
    "5. Repeat steps 4 until the agent is sufficiently trained.\n",
    "\"\"\"\n",
    "losses = []\n",
    "policies = [compute_avg_return(train_env, agent.policy, 10)]\n",
    "\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Agent will output loss every {log_interval} steps.\")\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    experience = next(iterator)\n",
    "    train_loss = agent.train(experience[0]).loss\n",
    "    \"\"\" if _ % 100 == 0:  # Print every 100 steps\n",
    "    print(f'Step: {_} \\t , Loss: {train_loss}') \"\"\"\n",
    "    if _ % log_interval == 0: \n",
    "        # print(\n",
    "        #     f'Step: {_} \\t , Loss: {train_loss}, Policy Measure: {compute_avg_return(train_env, agent.policy, 10)}')\n",
    "        print(f'Step: {_} \\t , Loss: {train_loss:.2f}')\n",
    "        losses.append(train_loss)\n",
    "        policies.append(compute_avg_return(train_env, driver.policy, 10))\n",
    "    if train_loss < 0.005 or train_loss > 10000:\n",
    "        print(f\"Training loss hit {train_loss}\")\n",
    "        break\n",
    "\n",
    "\"\"\" X = range(0, len(losses) * log_interval)\n",
    "Y = losses\n",
    "plt.plot(X, Y)\n",
    "plt.ylim(top=550) \"\"\"\n",
    "\n",
    "print(losses)\n",
    "print(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
