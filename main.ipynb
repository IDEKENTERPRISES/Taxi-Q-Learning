{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4049 Assessment 2:\n",
    "\n",
    "This assessment requires the use of a Taxi environment to train a model, using OpenAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # For the environment.\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can break down reinforcement learning into five simple steps:__\n",
    "\n",
    "1. The agent is at state zero in an environment.\n",
    "2. It will take an action based on a specific strategy.\n",
    "3. It will receive a reward or punishment based on that action.\n",
    "4. By learning from previous moves the the strategy of the agent becomes optimised. \n",
    "5. The process will repeat until an optimal strategy is found. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon-greedy or $\\epsilon$-greedy method balances the exploration of an environment with a probability $\\epsilon \\approx 10 \\% $ and the exploitation of an environment, with probability $1-\\epsilon$ at the same time. \n",
    "\n",
    "We start with a higher $\\epsilon$, which reduces over time due to understanding the environment better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Method for the TaxiAgent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiAgent:\n",
    "    def __init__(self, gamma: float = 0.95, alpha: float = 0.7, currentEpsilon: float = 1.0, decayFactor: float = 0.1):\n",
    "        \"\"\"An agent to be used for the taxi. This will keep track of the state of the taxi. This takes in 4 values, the gamma or the discount factor, the alpha or the learning rate, the current epsilon(the factor that controls the rate of exploration), and the decay factor which controls the rate at which the epsilon reduces.\"\"\"\n",
    "        self.env = gym.make('Taxi-v3')\n",
    "        state_space = self.env.observation_space.n\n",
    "        action_space = self.env.action_space.n\n",
    "        print(state_space, action_space)\n",
    "        self.quality_matrix = np.zeros((state_space, action_space))\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.currentEpsilon = currentEpsilon\n",
    "        self.minEpsilon = decayFactor\n",
    "        self.reset()\n",
    "        \"\"\" print(env.action_space.n) \"\"\"\n",
    "        \"\"\" print(f'Random action = {env.action_space.sample()} ') \"\"\"\n",
    "        \"\"\" print(observation) \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def chooseAction(self, observation) -> int:\n",
    "        \"\"\"Choose the action based on the epsilon greedy principle.\"\"\"\n",
    "        greediness = random.uniform(0, 1)\n",
    "        if greediness > self.currentEpsilon:\n",
    "            # Agent has chosen to exploit the environment\n",
    "            action = np.argmax(self.quality_matrix[observation])\n",
    "        else:\n",
    "            # Agent has chosen to explore the environment\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        self.observation, self.info = self.env.reset()\n",
    "\n",
    "    def updateQualityMatrix(self, action: int, old_obs: int, new_obs: int, reward) -> None:\n",
    "        \"\"\"Internally updates the QMatrix using the Bellman equation.\"\"\"\n",
    "        self.quality_matrix[old_obs][action] += self.alpha*(reward+(self.gamma*np.max(\n",
    "            self.quality_matrix[new_obs]) - self.quality_matrix[old_obs][action]))\n",
    "\n",
    "    def decayEpsilon(self, episode: int) -> None:\n",
    "        \"\"\"A function that changes the epsilon amount to be smaller, reflecting the decrease in exploration.\"\"\"\n",
    "        self.currentEpsilon = self.minEpsilon + \\\n",
    "            (1 - self.minEpsilon)*np.exp(-self.gamma*episode)\n",
    "\n",
    "    def step(self, action) -> bool:\n",
    "        \"\"\"New step function using the QMatrix. Will output True if the environment is terminated or finishes.\"\"\"\n",
    "        new_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.updateQualityMatrix(action, self.observation, new_obs, reward)\n",
    "        self.observation = new_obs\n",
    "        return terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 6\n"
     ]
    }
   ],
   "source": [
    "def train(episodes: int, max_steps: int = 200):\n",
    "    \"\"\"The function to train the TaxiAgent.\"\"\"\n",
    "    agent = TaxiAgent()\n",
    "    for episode in range(episodes):\n",
    "        agent.reset()\n",
    "        agent.decayEpsilon(episode)\n",
    "        curr_step = 1\n",
    "        done = False\n",
    "        while curr_step < max_steps:\n",
    "            action_to_take = agent.chooseAction(agent.observation)\n",
    "            done = agent.step(action_to_take)\n",
    "            curr_step += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "resulting_agent = train(2000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward= 7.91 \n",
      " ± std of: 2.62\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(env: gym.Env, max_steps: int, numEvalEpisodes: int, Q: np.array):\n",
    "    \"\"\"This function evaluates the agent environment and outputs the mean reward and the standard deviation reward for the environment.\"\"\"\n",
    "\n",
    "    episode_rewards = []\n",
    "    for episode in range(numEvalEpisodes):\n",
    "        state, _ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take the action (index) that have the maximum reward\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done1, done2, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done1 or done2:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_agent(\n",
    "    resulting_agent.env, 200, 1000, resulting_agent.quality_matrix)\n",
    "print(f\"Mean reward= {mean_reward:.2f} \\n ± std of: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_agent(env: gym.Env, max_steps: int, Q: np.array) -> None:\n",
    "    \"\"\"This is a visualising function for the environment.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    rewards = []\n",
    "    for step in range(max_steps):\n",
    "        # Take the action (index) that have the maximum reward\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done1, done2, info = env.step(action)\n",
    "        total_rewards_ep += reward\n",
    "        rewards.append(total_rewards_ep)\n",
    "\n",
    "        if done1 or done2:\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "\n",
    "new_env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "watch_agent(new_env, 200, resulting_agent.quality_matrix)\n",
    "new_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning Method for Taxi Agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning model uses multiple layers of a neural network to extract the abstract data from an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "This version of TensorFlow Probability requires TensorFlow version >= 2.15; Detected an installation of version 2.14.0. Please upgrade TensorFlow to proceed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\main.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juju-/OneDrive%20-%20University%20of%20Aberdeen/CS4049/Assessment%202/CS4049-assessment-2/main.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\" Have an agent class, with a policy. \"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juju-/OneDrive%20-%20University%20of%20Aberdeen/CS4049/Assessment%202/CS4049-assessment-2/main.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m \u001b[39mimport\u001b[39;00m suite_gym\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juju-/OneDrive%20-%20University%20of%20Aberdeen/CS4049/Assessment%202/CS4049-assessment-2/main.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_py_environment\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juju-/OneDrive%20-%20University%20of%20Aberdeen/CS4049/Assessment%202/CS4049-assessment-2/main.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdqn\u001b[39;00m \u001b[39mimport\u001b[39;00m dqn_agent\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\environments\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Copyright 2018 The TF-Agents Authors.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Environments module.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m \u001b[39mimport\u001b[39;00m batched_py_environment\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m \u001b[39mimport\u001b[39;00m parallel_py_environment\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m \u001b[39mimport\u001b[39;00m py_environment\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\environments\\batched_py_environment.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgin\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m  \u001b[39m# pylint: disable=g-explicit-tensorflow-version-import\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m \u001b[39mimport\u001b[39;00m py_environment\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m nest_utils\n\u001b[0;32m     36\u001b[0m \u001b[39m@gin\u001b[39m\u001b[39m.\u001b[39mconfigurable\n\u001b[0;32m     37\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBatchedPyEnvironment\u001b[39;00m(py_environment\u001b[39m.\u001b[39mPyEnvironment):\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\environments\\py_environment.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mabc\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrajectories\u001b[39;00m \u001b[39mimport\u001b[39;00m time_step \u001b[39mas\u001b[39;00m ts\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m common\n\u001b[0;32m     33\u001b[0m \u001b[39m@six\u001b[39m\u001b[39m.\u001b[39madd_metaclass(abc\u001b[39m.\u001b[39mABCMeta)\n\u001b[0;32m     34\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPyEnvironment\u001b[39;00m(\u001b[39mobject\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\trajectories\\__init__.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Trajectories module.\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrajectories\u001b[39;00m \u001b[39mimport\u001b[39;00m policy_step\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrajectories\u001b[39;00m \u001b[39mimport\u001b[39;00m time_step\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrajectories\u001b[39;00m \u001b[39mimport\u001b[39;00m trajectory\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\trajectories\\time_step.py:28\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m  \u001b[39m# pylint: disable=g-explicit-tensorflow-version-import\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecs\u001b[39;00m \u001b[39mimport\u001b[39;00m array_spec\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecs\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_spec\n\u001b[0;32m     31\u001b[0m _as_float32_array \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(np\u001b[39m.\u001b[39masarray, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\specs\\__init__.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m ArraySpec\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m BoundedArraySpec\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribution_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m DistributionSpec\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensor_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m BoundedTensorSpec\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_agents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensor_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorSpec\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tf_agents\\specs\\distribution_spec.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m---> 22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nest  \u001b[39m# pylint:disable=g-direct-tensorflow-import  # TF internal\u001b[39;00m\n\u001b[0;32m     26\u001b[0m tfd \u001b[39m=\u001b[39m tfp\u001b[39m.\u001b[39mdistributions\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tensorflow_probability\\__init__.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mimport\u001b[39;00m substrates\n\u001b[0;32m     21\u001b[0m \u001b[39m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     26\u001b[0m \u001b[39m# tfp_google.bind(globals())  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m# del tfp_google  # DisableOnExport\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:138\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m _tf_loaded():\n\u001b[0;32m    136\u001b[0m   \u001b[39m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m   \u001b[39mfor\u001b[39;00m pkg_name \u001b[39min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mdir\u001b[39;49m(\u001b[39mglobals\u001b[39;49m()[pkg_name])  \u001b[39m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m all_util\u001b[39m.\u001b[39mremove_undocumented(\u001b[39m__name__\u001b[39m, _lazy_load \u001b[39m+\u001b[39m _maybe_nonlazy_load)\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:57\u001b[0m, in \u001b[0;36mLazyLoader.__dir__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__dir__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 57\u001b[0m   module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n\u001b[0;32m     58\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mdir\u001b[39m(module)\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:37\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_first_access):\n\u001b[1;32m---> 37\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_first_access()\n\u001b[0;32m     38\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_first_access \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juju-\\OneDrive - University of Aberdeen\\CS4049\\Assessment 2\\CS4049-assessment-2\\venv\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:59\u001b[0m, in \u001b[0;36m_validate_tf_environment\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m#   required_tensorflow_version = '1.15'  # Needed internally -- DisableOnExport\u001b[39;00m\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m (distutils\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39mLooseVersion(tf\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m\n\u001b[0;32m     58\u001b[0m       distutils\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39mLooseVersion(required_tensorflow_version)):\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     60\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mThis version of TensorFlow Probability requires TensorFlow \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mversion >= \u001b[39m\u001b[39m{required}\u001b[39;00m\u001b[39m; Detected an installation of version \u001b[39m\u001b[39m{present}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     62\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mPlease upgrade TensorFlow to proceed.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     63\u001b[0m             required\u001b[39m=\u001b[39mrequired_tensorflow_version,\n\u001b[0;32m     64\u001b[0m             present\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39m__version__))\n\u001b[0;32m     66\u001b[0m   \u001b[39mif\u001b[39;00m (package \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmcmc\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m     67\u001b[0m       tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mtensor_float_32_execution_enabled()):\n\u001b[0;32m     68\u001b[0m     \u001b[39m# Must import here, because symbols get pruned to __all__.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: This version of TensorFlow Probability requires TensorFlow version >= 2.15; Detected an installation of version 2.14.0. Please upgrade TensorFlow to proceed."
     ]
    }
   ],
   "source": [
    "\"\"\" Have an agent class, with a policy. \"\"\"\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "\n",
    "\n",
    "\n",
    "class RLAgent():\n",
    "  def __init__(self) -> None:\n",
    "    self.env_name = 'Taxi-v3'\n",
    "    \"\"\" self.env = gym.make('Taxi-v3') \"\"\"\n",
    "    self.env = suite_gym.load(self.env_name)\n",
    "\n",
    "    train_py_env = suite_gym.load(self.env_name)\n",
    "    eval_py_env = suite_gym.load(self.env_name)\n",
    "    train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "    train_env = tf_py_environment.TFPyEnvironment(self.env_name)\n",
    "    pass\n",
    "  def policy(self):\n",
    "    pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2375627176.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    super.__init__():\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class QRLAgent(RLAgent):\n",
    "  \"\"\"Q Learning agent. Uses epsilon greedy method for the secondary policy. \"\"\"\n",
    "  def __init__(self):\n",
    "    super.__init__()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
